{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "556 Term Project.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "nzgykGww94sa",
        "JQF_75G2Bn-A",
        "CBkCMwFj99bX",
        "pyqrJNG9PnKk",
        "nmsQkxCiV6MS",
        "OwCy5dozUoBk",
        "I3xYLAfLfCq8",
        "4djY8diofHFC",
        "AcBgeUAdyyRn",
        "mvAkh_LjxPcG",
        "ehxz6PaJ-f16",
        "tCUhiVVpexS-",
        "zIIs4-N1gk2p",
        "M9YCsfKFFYqq",
        "d2cHwZcXFYq2",
        "VyqzggffFYq3",
        "F26DtSpPzCHa",
        "HWtmeWpGzE0o",
        "6k1bVne-FYq4",
        "7t7tYzo2ILaA",
        "6fjUVzLKOlIa"
      ],
      "authorship_tag": "ABX9TyM9EG6vZMnRFN3SvLGMBy4m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HongFFF/OPTI-556/blob/main/556_Term_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2QNO4LH90Px"
      },
      "source": [
        "# OPTI 556 Term Project: A Neural Network that Improves the Image Quality of a List-Mode CT/SPECT Imaging System\n",
        "### Yifan Hong\n",
        "\n",
        "In the term project, I plan to use convolutional neural networks to improve the reconstructed image quality for a CT imaging system that uses list-mode data. But after some investigation on the math and code, I found that the SPECT system is more practical as the computation time in the forward model is much smaller than the CT system.\n",
        "\n",
        "List mode data has a lot of advantages, like better reconstruction accuracy, less storage needed, and more adaptability in the computation. However, the major problem is that the reconstruction needs much higher computation power than the traditional dataset. The complexity of reconstruction algorithm is linear depend on the number of photons, which is normally a large number in one measurement. So in this project I plan to use a convolutional neural network to improve the image quality with limit number of photons.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzgykGww94sa"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wcov0b1YKRwE"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from skimage.restoration import denoise_tv_chambolle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQF_75G2Bn-A"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zitx7u8PwnM"
      },
      "source": [
        "def genData():\n",
        "  ### Forward\n",
        "  ptheta = np.zeros((Nphoton,2))\n",
        "  Nellipse = np.random.poisson(lam = Nellipse_mean)+1\n",
        "  A = np.zeros(Nellipse)\n",
        "  B = np.zeros(Nellipse)\n",
        "  beta = np.zeros(Nellipse)\n",
        "  xs = np.zeros(Nellipse)\n",
        "  ys = np.zeros(Nellipse)\n",
        "  area = np.zeros(Nellipse)\n",
        "  Cumarea = np.zeros(Nellipse)\n",
        "  for i in range(Nellipse):\n",
        "    A[i], B[i], beta[i], xs[i], ys[i] = gen_ellipse(A_mean, B_mean, Lxs, Lys)\n",
        "    area[i] = A[i]*B[i]\n",
        "    Cumarea[i] = area[i]\n",
        "    if i>0:\n",
        "      Cumarea[i] = Cumarea[i] + Cumarea[i-1]\n",
        "  Cumarea = Cumarea/np.sum(area)\n",
        "  for i in range(Nphoton):\n",
        "    Nsource = Nellipse - sum(np.random.random()<Cumarea)\n",
        "    ptheta[i:i+1,:] = gen_photon_ptheta(A[Nsource], B[Nsource], beta[Nsource], xs[Nsource], ys[Nsource])\n",
        "  ### Golden Truth\n",
        "  obj_true = np.zeros(X.shape)\n",
        "  for i in range(Nellipse):\n",
        "    obj_true = obj_true+plot_ellipse(A[i],B[i],beta[i],xs[i],ys[i],X,Y)\n",
        "  ### FBP\n",
        "  obj_recon = np.zeros(X.shape)\n",
        "  for i in range(Nphoton):\n",
        "    obj_recon = obj_recon+backproj(ptheta[i,0],ptheta[i,1],epsilon, X, Y)\n",
        "  ### Normalization\n",
        "  obj_true = obj_true/np.max(obj_true)\n",
        "  obj_recon = obj_recon/np.max(obj_recon)\n",
        "  return obj_recon, obj_true\n",
        "\n",
        "def genDatawithTV():\n",
        "  ### Forward\n",
        "  ptheta = np.zeros((Nphoton,2))\n",
        "  Nellipse = np.random.poisson(lam = Nellipse_mean)+1\n",
        "  A = np.zeros(Nellipse)\n",
        "  B = np.zeros(Nellipse)\n",
        "  beta = np.zeros(Nellipse)\n",
        "  xs = np.zeros(Nellipse)\n",
        "  ys = np.zeros(Nellipse)\n",
        "  area = np.zeros(Nellipse)\n",
        "  Cumarea = np.zeros(Nellipse)\n",
        "  for i in range(Nellipse):\n",
        "    A[i], B[i], beta[i], xs[i], ys[i] = gen_ellipse(A_mean, B_mean, Lxs, Lys)\n",
        "    area[i] = A[i]*B[i]\n",
        "    Cumarea[i] = area[i]\n",
        "    if i>0:\n",
        "      Cumarea[i] = Cumarea[i] + Cumarea[i-1]\n",
        "  Cumarea = Cumarea/np.sum(area)\n",
        "  for i in range(Nphoton):\n",
        "    Nsource = Nellipse - sum(np.random.random()<Cumarea)\n",
        "    ptheta[i:i+1,:] = gen_photon_ptheta(A[Nsource], B[Nsource], beta[Nsource], xs[Nsource], ys[Nsource])\n",
        "  ### Golden Truth\n",
        "  obj_true = np.zeros(X.shape)\n",
        "  for i in range(Nellipse):\n",
        "    obj_true = obj_true+plot_ellipse(A[i],B[i],beta[i],xs[i],ys[i],X,Y)\n",
        "  ### FBP\n",
        "  obj_recon = np.zeros(X.shape)\n",
        "  for i in range(Nphoton):\n",
        "    obj_recon = obj_recon+backproj(ptheta[i,0],ptheta[i,1],epsilon, X, Y)  \n",
        "  obj_recon = obj_recon/np.max(obj_recon)\n",
        "  obj_recon_orig = obj_recon\n",
        "  obj_recon = denoise_tv_chambolle(obj_recon, weight = 0.5)\n",
        "  ### Normalization\n",
        "  obj_true = obj_true/np.max(obj_true)\n",
        "  obj_recon = obj_recon/np.max(obj_recon)\n",
        "  return obj_recon, obj_true, obj_recon_orig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SwqwTmpBnRW"
      },
      "source": [
        "def gen_ellipse(A_mean, B_mean, Lxs, Lys):\n",
        "  A = abs(np.random.randn()*A_mean+A_mean)\n",
        "  B = abs(np.random.randn()*B_mean+B_mean)\n",
        "  beta = np.random.random()*np.pi\n",
        "  xs = np.random.random()*Lxs-Lxs/2\n",
        "  ys = np.random.random()*Lys-Lys/2\n",
        "  return A, B, beta, xs, ys\n",
        "\n",
        "def plot_ellipse(A,B,beta,xs,ys,X,Y):\n",
        "  obj = np.zeros(X.shape)\n",
        "  obj = ((X+xs)*np.cos(beta)-(Y+ys)*np.sin(beta))**2/A**2+((X+xs)*np.sin(beta)+(Y+ys)*np.cos(beta))**2/B**2\n",
        "  obj = obj<=1\n",
        "  obj = obj.astype(float)\n",
        "  return obj\n",
        "\n",
        "def gen_photon_ptheta(A,B,beta,xs,ys):\n",
        "  theta = np.random.random()*2*np.pi\n",
        "  p =  np.sqrt(np.random.random())\n",
        "  x,y = ptheta2xy(p,theta)\n",
        "  x = x*A\n",
        "  y = y*B\n",
        "  p, theta = xy2ptheta(x,y)\n",
        "  theta = theta-beta\n",
        "  x, y = ptheta2xy(p, theta)\n",
        "  x = x+xs\n",
        "  y = y+ys\n",
        "  theta = np.random.random()*np.pi\n",
        "  p = -x*np.cos(theta)-y*np.sin(theta)\n",
        "  return [p, theta]\n",
        "\n",
        "def backproj(p, theta, epsilon, X, Y):\n",
        "  dist = np.abs(p-X*np.cos(theta)-Y*np.sin(theta))\n",
        "  dist2 = dist**2\n",
        "  return -(2*dist2)/(dist2+epsilon**2)**2+1/(dist2+epsilon**2)\n",
        "\n",
        "#def backproj(p, theta, epsilon, X, Y):\n",
        "#  dx = X[0,1]-X[0,0]\n",
        "#  dy = Y[1,0]-Y[0,0]\n",
        "#  e2 = epsilon**2\n",
        "#  Z1 = X*np.cos(theta)+(Y+dy)*np.sin(theta)-p\n",
        "#  Z2 = X*np.cos(theta)+Y*np.sin(theta)-p\n",
        "#  Z3 = (X+dx)*np.cos(theta)+Y*np.sin(theta)-p\n",
        "#  Z4 = (X+dx)*np.cos(theta)+(Y+dy)*np.sin(theta)-p\n",
        "#  Int = np.log(Z1**2+e2)-np.log(Z2**2+e2)+np.log(Z3**2+e2)-np.log(Z4**2+e2)\n",
        "#  const = -1/(2*np.cos(theta)*np.sin(theta))\n",
        "#  return Int*const\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5izp5ubaHyf"
      },
      "source": [
        "#### Less Major Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq97iHujaEC8"
      },
      "source": [
        "def ptheta2xy(p,theta):\n",
        "  x = p*np.cos(theta)\n",
        "  y = p*np.sin(theta)\n",
        "  return x, y\n",
        "\n",
        "def xy2ptheta(x,y):\n",
        "  if y>0:\n",
        "    sign = 1\n",
        "  else:\n",
        "    sign = -1\n",
        "  p = np.sqrt(x**2+y**2)*sign\n",
        "  x = x*sign\n",
        "  theta = math.acos(x/np.sqrt(x**2+y**2))\n",
        "  return p, theta\n",
        "\n",
        "def calc_prob(A,B,beta,alpha,xs,ys):\n",
        "  factor1 = (1/A**2+1/B**2)\n",
        "  termA = factor1**2*(xs*math.cos(alpha)*math.cos(beta)**2+ys*math.sin(alpha)*math.cos(beta)**2-math.cos(alpha)*math.cos(beta)-math.sin(alpha)*math.sin(beta))*2\n",
        "  termB = factor1*(math.cos(alpha)**2*math.cos(beta)**2+math.sin(alpha)**2*math.sin(beta)**2)\n",
        "  termC = factor1*(xs**2*math.cos(beta)+ys**2*math.sin(beta)-2*xs*math.cos(beta)-2*ys*math.sin(beta))-1\n",
        "  ALongOne = A-B*C\n",
        "  if ALongOne>0:\n",
        "    Lpath = 2*np.sqrt(ALongOne)/factor1/(math.cos(alpha)**2*math.cos(beta)**2-math.sin(alpha)**2*math.sin(beta)**2)\n",
        "  else:\n",
        "    Lpath = 0\n",
        "  return np.exp(-mu*Lpath)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBkCMwFj99bX"
      },
      "source": [
        "### Parameters\n",
        "Most of the parameters are shown as below. The mean number of ellipse is chosen as 5, and the true number is distributed as a Poisson distribution. The long/short axes use 0.2 as the mean, where the computational window is 2 by 2 with 128 pixels on each side. The origin of the ellipses located at the center in a 1 by 1 area. The orientation is uniformly distributed in pi angle. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Bccohu9_Lt"
      },
      "source": [
        "Lx = 2\n",
        "Ly = 2\n",
        "Lp = 3\n",
        "Lxs = 1\n",
        "Lys = 1\n",
        "\n",
        "Nx = 128\n",
        "Ny = 128\n",
        "Lsource = 1\n",
        "Nphoton= 10000\n",
        "\n",
        "Nellipse_mean = 5\n",
        "A_mean = 0.2\n",
        "B_mean = 0.2\n",
        "mu_mean = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn1XH9EwFNk5"
      },
      "source": [
        "x = np.linspace(-Lx/2,Lx/2,Nx+1)\n",
        "x = x[0:Nx]\n",
        "y = np.linspace(-Ly/2,Ly/2,Ny+1)\n",
        "y = y[0:Ny]\n",
        "dx = x[2]-x[1]\n",
        "dy = y[2]-y[1]\n",
        "X, Y = np.meshgrid(x,y)\n",
        "\n",
        "epsilon = 1*np.sqrt(dx*dy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9c1WO0wGMUE"
      },
      "source": [
        "### Some test on the Forward and Backward model\n",
        "In this section, I briefly tested on the results of the object generator. And then the random object was used to show the validation of the forward and backward model of the SPECT system. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyqrJNG9PnKk"
      },
      "source": [
        "#### Forward Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlEByPa1GLu_"
      },
      "source": [
        "ptheta = np.zeros((Nphoton,2))\n",
        "\n",
        "Nellipse = np.random.poisson(lam = Nellipse_mean)+1\n",
        "#\n",
        "#Nellipse = 1\n",
        "#\n",
        "A = np.zeros(Nellipse)\n",
        "B = np.zeros(Nellipse)\n",
        "beta = np.zeros(Nellipse)\n",
        "xs = np.zeros(Nellipse)\n",
        "ys = np.zeros(Nellipse)\n",
        "area = np.zeros(Nellipse)\n",
        "Cumarea = np.zeros(Nellipse)\n",
        "\n",
        "for i in range(Nellipse):\n",
        "  A[i], B[i], beta[i], xs[i], ys[i] = gen_ellipse(A_mean, B_mean, Lxs, Lys)\n",
        "  area[i] = A[i]*B[i]\n",
        "  Cumarea[i] = area[i]\n",
        "  if i>0:\n",
        "    Cumarea[i] = Cumarea[i] + Cumarea[i-1]\n",
        "Cumarea = Cumarea/np.sum(area)\n",
        "\n",
        "#\n",
        "#beta = np.zeros(Nellipse)\n",
        "#xs = np.zeros(Nellipse)\n",
        "#ys = np.zeros(Nellipse)\n",
        "#B = A\n",
        "#\n",
        "\n",
        "for i in range(Nphoton):\n",
        " Nsource = Nellipse - sum(np.random.random()<Cumarea)\n",
        " ptheta[i:i+1,:] = gen_photon_ptheta(A[Nsource], B[Nsource], beta[Nsource], xs[Nsource], ys[Nsource])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmsQkxCiV6MS"
      },
      "source": [
        "#### Print the object\n",
        "A random object from the ellipses generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ElaCpRCWARc"
      },
      "source": [
        "obj = np.zeros(X.shape)\n",
        "for i in range(Nellipse):\n",
        "  obj = obj+plot_ellipse(A[i],B[i],beta[i],xs[i],ys[i],X,Y)\n",
        "plt.figure()\n",
        "plt.imshow(obj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwCy5dozUoBk"
      },
      "source": [
        "#### FBP\n",
        "The estimated object with 10k photons with the list mode data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SlUXgcTUnbW"
      },
      "source": [
        "obj = np.zeros(X.shape)\n",
        "for i in range(Nphoton):\n",
        "  obj = obj+backproj(ptheta[i,0],ptheta[i,1],epsilon, X, Y)\n",
        "plt.figure()\n",
        "obj = obj/np.max(obj)\n",
        "obj = denoise_tv_chambolle(obj, weight = 0.2)\n",
        "plt.imshow(obj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9VOHraLUiN4"
      },
      "source": [
        "### Generate Dataset without Total Variation regularization\n",
        "\n",
        "Here is the first result, which the inputs to the neural network are directly calculated from the FBP. 1000 object are used as the training dataset and 100 objects are used as the validation dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3xYLAfLfCq8"
      },
      "source": [
        "#### Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0AoqFSYUhUE"
      },
      "source": [
        "Ntrain = 1000\n",
        "Ntest = 100\n",
        "\n",
        "xtrain = np.zeros((Ntrain,Nx,Ny))\n",
        "ytrain = np.zeros((Ntrain,Nx,Ny))\n",
        "xtest = np.zeros((Ntest,Nx,Ny))\n",
        "ytest = np.zeros((Ntest,Nx,Ny))\n",
        "\n",
        "for i in range(Ntest):\n",
        "  xtest[i:i+1,:,:], ytest[i:i+1,:,:] = genData()\n",
        "for i in range(Ntrain):\n",
        "  xtrain[i:i+1,:,:], ytrain[i:i+1,:,:] = genData()\n",
        "\n",
        "xtrain = np.reshape(xtrain, (len(xtrain), Nx, Ny, 1))\n",
        "ytrain = np.reshape(ytrain, (len(ytrain), Nx, Ny, 1))\n",
        "xtest = np.reshape(xtest, (len(xtest), Nx, Ny, 1))\n",
        "ytest = np.reshape(ytest, (len(ytest), Nx, Ny, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4djY8diofHFC"
      },
      "source": [
        "#### Or Load Data From Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsDOJqSBfGr7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "trainingData = np.load('drive/MyDrive/Colab Notebooks/DataSave/trainingData.npz')\n",
        "testData = np.load('drive/MyDrive/Colab Notebooks/DataSave/testData.npz')\n",
        "\n",
        "xtrain=trainingData['arr_0']\n",
        "ytrain=trainingData['arr_1']\n",
        "\n",
        "xtest=testData['arr_0']\n",
        "ytest=testData['arr_1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CNN1"
      ],
      "metadata": {
        "id": "AcBgeUAdyyRn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hPA-kjUY2kF"
      },
      "source": [
        "inputs=keras.Input(shape=(Nx, Ny, 1))\n",
        "\n",
        "# Encoder\n",
        "x = layers.Conv2D(64, (3, 3), strides=1, activation=\"relu\", padding=\"same\")(inputs)\n",
        "x1 = layers.Conv2D(128, (8, 8), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x1)\n",
        "x = layers.Conv2D(256, (3, 3), strides=1, activation=\"relu\", padding=\"same\")(x)\n",
        "\n",
        "\n",
        "# Decoder\n",
        "x2 = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Add()([x1,x2])\n",
        "x = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(16, (4, 4), strides=1, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(8, (8, 8), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(1, (3, 3), activation=\"linear\", padding=\"same\")(x)\n",
        "\n",
        "SPECT_denoise = keras.Model(inputs, x)\n",
        "SPECT_denoise.compile(optimizer='adam', loss='mean_squared_error')\n",
        "SPECT_denoise.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CNN2"
      ],
      "metadata": {
        "id": "mvAkh_LjxPcG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkgPKa9MiLla"
      },
      "source": [
        "inputs=keras.Input(shape=(Nx, Ny, 1))\n",
        "\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# At this point the representation is (7, 7, 32)\n",
        "\n",
        "x = layers.Conv2DTranspose(32, (3, 3), activation='relu',strides= 2, padding='same')(encoded)\n",
        "x = layers.Conv2DTranspose(32, (3, 3), activation='relu',strides= 2, padding='same')(x)\n",
        "decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "SPECT_denoise = keras.Model(inputs, decoded)\n",
        "SPECT_denoise.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "SPECT_denoise.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-AoDin5b8hL"
      },
      "source": [
        "SPECT_denoise.fit(\n",
        "    x=xtrain,\n",
        "    y=ytrain,\n",
        "    epochs=20,\n",
        "    batch_size=100,\n",
        "    shuffle=True,\n",
        "    validation_data=(xtest, ytest),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehxz6PaJ-f16"
      },
      "source": [
        "##### Save Data to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN98BXXIDHq1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dVe6LtW-i4E"
      },
      "source": [
        "np.savez('drive/MyDrive/Colab Notebooks/DataSave/trainingData',xtrain,ytrain)\n",
        "np.savez('drive/MyDrive/Colab Notebooks/DataSave/testData',xtest,ytest)\n",
        "SPECT_denoise.save('drive/MyDrive/Colab Notebooks/DataSave/SPECT_denoise') \n",
        "\n",
        "\n",
        "SPECT_denoise.save(\"SPECT_denoise\")\n",
        "!tar -czvf SPECT_denoise.tar.gz SPECT_denoise/\n",
        "\n",
        "#files.download('testData.npz')\n",
        "#files.download('trainingData.npz')\n",
        "#files.download('SPECT_denoise.tar.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCUhiVVpexS-"
      },
      "source": [
        "##### Load Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSx75dtv_3CI"
      },
      "source": [
        "SPECT_denoise = keras.models.load_model('drive/MyDrive/Colab Notebooks/DataSave/SPECT_denoise')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIIs4-N1gk2p"
      },
      "source": [
        "### Results Plot without Total Variation regularization\n",
        "Results from CNN: the first row are the golden truth, the second row are the result by FBP, and the last row are the estimation from the CNN\n",
        "\n",
        "From the result, we can observe that the neural network has a significant improvement on the reconstruction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1swvHgVgocf"
      },
      "source": [
        "n = 5\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(1, n + 1):\n",
        "    ax = plt.subplot(3, n, i)\n",
        "    plt.imshow(ytest[3*i].reshape(Nx, Ny))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    ax = plt.subplot(3, n, n+i)\n",
        "    plt.imshow(xtest[3*i].reshape(Nx, Ny))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    ax = plt.subplot(3, n, 2*n+i)\n",
        "    plt.imshow(SPECT_denoise.predict(xtest[3*i:3*i+1,:,:,:])[0,:,:,0])\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9YCsfKFFYqq"
      },
      "source": [
        "### Convolutional Neural Network with Total Variation Regularization\n",
        "Thanks to the advice received during the presentation, I added a total variation regularization before the input of the neural network. The weight is tuned based on the mse on the validation dataset. The results show that after the TV regularization, even with the same setup for the neural network, the estimation on the objects is better than the case without the TV regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2cHwZcXFYq2"
      },
      "source": [
        "#### Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15rE63p9FYq2"
      },
      "source": [
        "Ntrain = 1000\n",
        "Ntest = 100\n",
        "\n",
        "xtrain = np.zeros((Ntrain,Nx,Ny))\n",
        "ytrain = np.zeros((Ntrain,Nx,Ny))\n",
        "xtest = np.zeros((Ntest,Nx,Ny))\n",
        "ytest = np.zeros((Ntest,Nx,Ny))\n",
        "\n",
        "for i in range(Ntest):\n",
        "  xtest[i:i+1,:,:], ytest[i:i+1,:,:] = genDatawithTV()\n",
        "for i in range(Ntrain):\n",
        "  xtrain[i:i+1,:,:], ytrain[i:i+1,:,:] = genDatawithTV()\n",
        "\n",
        "xtrain = np.reshape(xtrain, (len(xtrain), Nx, Ny, 1))\n",
        "ytrain = np.reshape(ytrain, (len(ytrain), Nx, Ny, 1))\n",
        "xtest = np.reshape(xtest, (len(xtest), Nx, Ny, 1))\n",
        "ytest = np.reshape(ytest, (len(ytest), Nx, Ny, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyqzggffFYq3"
      },
      "source": [
        "#### Or Load Data From Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2-ReSliFYq3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "trainingData = np.load('drive/MyDrive/Colab Notebooks/DataSave/trainingDatawithTV2.npz')\n",
        "testData = np.load('drive/MyDrive/Colab Notebooks/DataSave/testDatawithTV2.npz')\n",
        "\n",
        "xtrain=trainingData['arr_0']\n",
        "ytrain=trainingData['arr_1']\n",
        "\n",
        "xtest=testData['arr_0']\n",
        "ytest=testData['arr_1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CNN1"
      ],
      "metadata": {
        "id": "F26DtSpPzCHa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOZgFJKjFYq3"
      },
      "source": [
        "inputs=keras.Input(shape=(Nx, Ny, 1))\n",
        "\n",
        "# Encoder\n",
        "x = layers.Conv2D(64, (3, 3), strides=1, activation=\"relu\", padding=\"same\")(inputs)\n",
        "x1 = layers.Conv2D(128, (8, 8), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x1)\n",
        "x = layers.Conv2D(256, (3, 3), strides=1, activation=\"relu\", padding=\"same\")(x)\n",
        "\n",
        "\n",
        "# Decoder\n",
        "x2 = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Add()([x1,x2])\n",
        "x = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(16, (4, 4), strides=1, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(8, (8, 8), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(1, (3, 3), activation=\"linear\", padding=\"same\")(x)\n",
        "\n",
        "SPECT_denoise = keras.Model(inputs, x)\n",
        "SPECT_denoise.compile(optimizer='adam', loss='mean_squared_error')\n",
        "SPECT_denoise.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CNN2"
      ],
      "metadata": {
        "id": "HWtmeWpGzE0o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmBPnjeMFYq3"
      },
      "source": [
        "inputs=keras.Input(shape=(Nx, Ny, 1))\n",
        "\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# At this point the representation is (7, 7, 32)\n",
        "\n",
        "x = layers.Conv2DTranspose(32, (3, 3), activation='relu',strides= 2, padding='same')(encoded)\n",
        "x = layers.Conv2DTranspose(32, (3, 3), activation='relu',strides= 2, padding='same')(x)\n",
        "decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "SPECT_denoise = keras.Model(inputs, decoded)\n",
        "SPECT_denoise.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "SPECT_denoise.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okVV6RSnFYq4"
      },
      "source": [
        "SPECT_denoise.fit(\n",
        "    x=xtrain,\n",
        "    y=ytrain,\n",
        "    epochs=20,\n",
        "    batch_size=100,\n",
        "    shuffle=True,\n",
        "    validation_data=(xtest, ytest),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k1bVne-FYq4"
      },
      "source": [
        "#### Save Data to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-Yhk8TfFYq4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiHZwTOSFYq4"
      },
      "source": [
        "np.savez('drive/MyDrive/Colab Notebooks/DataSave/trainingDatawithTV2',xtrain,ytrain)\n",
        "np.savez('drive/MyDrive/Colab Notebooks/DataSave/testDatawithTV2',xtest,ytest)\n",
        "SPECT_denoise.save('drive/MyDrive/Colab Notebooks/DataSave/SPECT_denoise') \n",
        "\n",
        "\n",
        "SPECT_denoise.save(\"SPECT_denoise\")\n",
        "!tar -czvf SPECT_denoise.tar.gz SPECT_denoise/\n",
        "\n",
        "#files.download('testData.npz')\n",
        "#files.download('trainingData.npz')\n",
        "#files.download('SPECT_denoise.tar.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t7tYzo2ILaA"
      },
      "source": [
        "### Results Plot with Total Variation Regularization\n",
        "Results from CNN: the first row are the golden truth, \n",
        "the second row are the result by FBP, the third row are the outputs of TV regularization, and the last row are the estimation from the CNN\n",
        "\n",
        "From the result we can observe that the TV regularization can already decrease the artifact by a lot, and the CNN will further increase the sharpness and clear the background. It might be hard to observe the difference between the case with and without the TV regularization. But from the MSE during the training, with the same neural network, having the TV regularization can decrease the MSE by a little. \n",
        "\n",
        "I am still trying to increase the size of the training data, but due to the limitation on resource, it might be infeasible to do it on the Colab. I will try to implement it on other platform. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_Zi4rDBILaA"
      },
      "source": [
        "n = 5\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(1, n + 1):\n",
        "    ax = plt.subplot(4, n, i)\n",
        "    plt.imshow(ytest[i].reshape(Nx, Ny))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    ax = plt.subplot(4, n, n+i)\n",
        "    plt.imshow(xtest_orig[i].reshape(Nx, Ny))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    ax = plt.subplot(4, n, 2*n+i)\n",
        "    plt.imshow(xtest[i].reshape(Nx, Ny))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    ax = plt.subplot(4, n, 3*n+i)\n",
        "    plt.imshow(SPECT_denoise.predict(xtest[i:i+1,:,:,:])[0,:,:,0])\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some Plots for the Presentation"
      ],
      "metadata": {
        "id": "6fjUVzLKOlIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate Test Data"
      ],
      "metadata": {
        "id": "nQ606ANw0xev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ntest = 10\n",
        "\n",
        "xtest = np.zeros((Ntest,Nx,Ny))\n",
        "ytest = np.zeros((Ntest,Nx,Ny))\n",
        "xtest_orig = np.zeros((Ntest,Nx,Ny))\n",
        "\n",
        "for i in range(Ntest):\n",
        "  xtest[i:i+1,:,:], ytest[i:i+1,:,:], xtest_orig[i:i+1,:,:]  = genDatawithTV()\n",
        "\n",
        "xtest = np.reshape(xtest, (len(xtest), Nx, Ny, 1))\n",
        "ytest = np.reshape(ytest, (len(ytest), Nx, Ny, 1))\n",
        "xtest_orig = np.reshape(xtest_orig, (len(xtest_orig), Nx, Ny, 1))"
      ],
      "metadata": {
        "id": "HQ9tSTypSuys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### True Objects"
      ],
      "metadata": {
        "id": "QGeNm48W0z5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i in range(1, n + 1):\n",
        "    ax = plt.subplot(1, n, i)\n",
        "    plt.imshow(ytest[i].reshape(Nx, Ny))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "id": "ZWt1mODrOkJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Estimated Objects from FBP"
      ],
      "metadata": {
        "id": "a28BS9bp03Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i in range(1, n + 1):\n",
        "    ax = plt.subplot(1, n, i)\n",
        "    plt.imshow(xtest_orig[i].reshape(Nx, Ny))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "id": "LbXi1Jj1TLTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Estimated Objects after TV Regularization"
      ],
      "metadata": {
        "id": "dusJQpJY06Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i in range(1, n + 1):\n",
        "    ax = plt.subplot(1, n, i)\n",
        "    plt.imshow(xtest[i].reshape(Nx, Ny))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "id": "Y9B6XIZAQbzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results from the Neural Network"
      ],
      "metadata": {
        "id": "1xMRptR51AXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(1, n + 1):\n",
        "    ax = plt.subplot(4, n, i)\n",
        "    plt.imshow(ytest[i].reshape(Nx, Ny))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    ax = plt.subplot(4, n, n+i)\n",
        "    plt.imshow(xtest_orig[i].reshape(Nx, Ny))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    ax = plt.subplot(4, n, 2*n+i)\n",
        "    plt.imshow(xtest[i].reshape(Nx, Ny))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    ax = plt.subplot(4, n, 3*n+i)\n",
        "    plt.imshow(SPECT_denoise.predict(xtest[i:i+1,:,:,:])[0,:,:,0])\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "id": "q5YY3FybSfjT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}